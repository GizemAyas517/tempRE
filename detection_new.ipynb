{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ca66578e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['asd,asd,asd']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\"asd,asd,asd\".split('q',1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "2cc6f607",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"dataset.xlsx\")\n",
    "df['PRONOUNS'] = df['PRONOUNS'].str.replace('it\\'s', 'it')\n",
    "df['USER STORIES'] = df['USER STORIES'].str.replace('it\\'s', 'it is')\n",
    "df['USER STORIES'] =df['USER STORIES'].apply(lambda x: x.split(',',1)[-1])  \n",
    "def resolution_func(x):\n",
    "    if type(x) == str:\n",
    "        return x.split(';')\n",
    "    return None\n",
    "\n",
    "df['PRONOUNS'] = df['PRONOUNS'].apply(lambda x:[a.strip()[1:-1] for a in x[1:-1].split(',') if a != ''])\n",
    "df['RESOLUTION'] = df['RESOLUTION'].apply(resolution_func)\n",
    "df = df[df['PRONOUNS'].map(len) != 0]\n",
    "#df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "2a62ad77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_grammar_tree(text):\n",
    "    grammar = r\"\"\"\n",
    "        NP:\n",
    "            {<DT>?<PRP.*>?<JJ>?<NN.*>*<IN>?<NN.*>}\n",
    "        NP: \n",
    "            {<NP><IN><NP>}\n",
    "        NP: \n",
    "            {<NP><IN><DT><VBG><NP>}\n",
    "        NP: \n",
    "            {<NP><CC><NP>}\n",
    "    \"\"\"\n",
    "    chunker = nltk.RegexpParser(grammar)\n",
    "    postoks = nltk.tag.pos_tag(word_tokenize(text))\n",
    "    tree = chunker.parse(postoks)\n",
    "    \n",
    "    def get_terms(tree):\n",
    "        ret = []\n",
    "        for i,t in enumerate(tree):\n",
    "            if type(t) == tuple:\n",
    "                ret.append((t[0], t[1]))\n",
    "            else:\n",
    "                ret.append((' '.join([w for w,tt in t.leaves()]), t.leaves()[-1][1]))\n",
    "        return ret\n",
    "\n",
    "    return get_terms(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "7952caf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pronoun_position(tree, pronouns):\n",
    "    \n",
    "    def _get_position(tree, p,  starting_index = 0):\n",
    "        for i,t in enumerate(tree[starting_index:]):\n",
    "\n",
    "            if t[0] == p:\n",
    "                return i + starting_index\n",
    "        return -1\n",
    "    \n",
    "    if len(pronouns) == 1:\n",
    "        return [_get_position(tree, pronouns[0])]\n",
    "    else:\n",
    "        index = 0\n",
    "        poss=[]\n",
    "        for pronoun in pronouns:\n",
    "            index = _get_position(tree, pronoun, starting_index = index)\n",
    "            poss.append(index)\n",
    "            index = index + 1\n",
    "        return poss\n",
    "\n",
    "# -------- FEATURE FUNCTIONS --------\n",
    "\n",
    "def get_plural_nouns(tree, nouns):\n",
    "    return [n for n in nouns if tree[n][1].endswith('S') or (tree[n][1] == \"NNP\" and tree[n][0].endswith('s'))]\n",
    "\n",
    "def get_singular_nouns(tree, nouns):       \n",
    "    return [n for n in nouns if not (tree[n][1].endswith('S') or (tree[n][1] == \"NNP\" and tree[n][0].endswith('s')))]\n",
    "\n",
    "def noun_count_before_pronoun(tree, pos, nouns):\n",
    "    count = 0\n",
    "    for n in nouns:\n",
    "        if n >= pos:\n",
    "            break\n",
    "        count += 1\n",
    "    return count\n",
    "\n",
    "def noun_count_after_pronoun(tree, pos, nouns):\n",
    "    count = 0\n",
    "    for n in nouns:\n",
    "        if n < pos:\n",
    "            continue\n",
    "        count += 1\n",
    "    return count\n",
    "\n",
    "def single_noun_before_pronoun(tree, pos, nouns):\n",
    "    count = 0\n",
    "    for n in nouns:\n",
    "        if n >= pos:\n",
    "            break\n",
    "        count += 1\n",
    "    return count == 1\n",
    "\n",
    "def only_one_same_quantity(tree, pos, nouns):\n",
    "    nouns = [n for n in nouns if n < pos]\n",
    "    if tree[pos][0].lower() == \"their\" or tree[pos][0].lower() == \"them\" or tree[pos][0].lower() == \"they\":\n",
    "        plural_nouns = get_plural_nouns(tree,nouns)\n",
    "        if len(plural_nouns) == 1:\n",
    "            return True\n",
    "    else:\n",
    "        singular_nouns = get_singular_nouns(tree,nouns)\n",
    "        if len(singular_nouns) == 1:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def is_there_initial_noun(nouns):\n",
    "    if len(nouns) > 0 and nouns[0] == 0:\n",
    "        return True\n",
    "    else: return False\n",
    "    \n",
    "def single_noun(nouns):\n",
    "    return len(nouns) == 1\n",
    "\n",
    "def pronoun_counts(pronoun_positions):\n",
    "    return len(pronoun_positions)\n",
    "\n",
    "def initial_pronoun(pos):\n",
    "    return pos == 0\n",
    "\n",
    "def special_noun(nouns, tree):\n",
    "    for n in nouns:\n",
    "        if tree[n][0].isupper():\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def special_single_noun(nouns, tree):\n",
    "    if len([tree[n][0].isupper() for n in nouns]) == 1:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def single_little_special_noun(nouns, tree):\n",
    "    if len([len([t for t in tree[n][0] if t.isupper()]) > 2 for n in nouns]) == 1:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def single_noun_with_the(nouns, tree):\n",
    "    if len([\"the\" in tree[n][0].lower() for n in nouns]) == 1:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def single_noun_with_a_an(nouns, tree):\n",
    "    if np.sum([\"a\" in tree[n][0].lower().split() for n in nouns]) == 1:\n",
    "        return True\n",
    "    elif np.sum([\"an\" in tree[n][0].lower().split() for n in nouns]) == 1:\n",
    "        return True\n",
    "    elif np.sum([\"my\"==tree[n-1][0].lower() for n in nouns]) == 1:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "# -------- FEATURE FUNCTIONS --------\n",
    "\n",
    "def get_feature_vector(tree, pronoun_position, nouns, pronoun_positions):\n",
    "    return [\n",
    "            single_noun_before_pronoun(tree,p,nouns),\n",
    "            noun_count_before_pronoun(tree,p,nouns),\n",
    "            noun_count_after_pronoun(tree,p,nouns),\n",
    "            only_one_same_quantity(tree,p,nouns),\n",
    "            is_there_initial_noun(nouns),\n",
    "            single_noun(nouns),\n",
    "            pronoun_counts(pronoun_positions),\n",
    "            initial_pronoun(pronoun_position),\n",
    "            special_noun(nouns, tree),\n",
    "            special_single_noun(nouns, tree),\n",
    "            single_little_special_noun(nouns, tree),\n",
    "            single_noun_with_the(nouns, tree),\n",
    "            single_noun_with_a_an(nouns, tree),\n",
    "          ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "242b273f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of train df 173\n"
     ]
    }
   ],
   "source": [
    "train_df, test_df = train_test_split(df, test_size=0.3, shuffle=False)\n",
    "\n",
    "X_train=[]\n",
    "y_train=[]\n",
    "print(\"size of train df %d\" % len(train_df))\n",
    "for i,d in train_df.iterrows():\n",
    "    tree = get_grammar_tree(d['USER STORIES'].strip())\n",
    "    pronouns = [p.strip() for p in d['PRONOUNS'] ]\n",
    "    pronoun_positions = get_pronoun_position(tree, pronouns)\n",
    "    nouns = [i for i,(t,p) in enumerate(tree) if p.startswith('NN')]\n",
    "    for p in pronoun_positions:\n",
    "        if d['AMBIGUITY'] == 'no':\n",
    "            X_train.append(get_feature_vector(tree, p, nouns, pronoun_positions))\n",
    "            y_train.append(0)\n",
    "        else: \n",
    "            X_train.append(get_feature_vector(tree, p, nouns, pronoun_positions))\n",
    "            y_train.append(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "3d84e5f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of test df: 75\n",
      "Nearest Neighbors:\t\t Accuracy: 0.45, Recall: 0.20, F1_score: 0.23, precision: 0.26\n",
      "Linear SVM:\t\t Accuracy: 0.63, Recall: 0.20, F1_score: 0.30, precision: 0.60\n",
      "RBF SVM:\t\t Accuracy: 0.43, Recall: 0.17, F1_score: 0.19, precision: 0.22\n",
      "Gaussian Process:\t\t Accuracy: 0.52, Recall: 0.27, F1_score: 0.31, precision: 0.36\n",
      "Decision Tree:\t\t Accuracy: 0.39, Recall: 0.10, F1_score: 0.12, precision: 0.14\n",
      "Random Forest:\t\t Accuracy: 0.53, Recall: 0.13, F1_score: 0.19, precision: 0.31\n",
      "Neural Net:\t\t Accuracy: 0.47, Recall: 0.20, F1_score: 0.23, precision: 0.27\n",
      "AdaBoost:\t\t Accuracy: 0.44, Recall: 0.23, F1_score: 0.25, precision: 0.27\n",
      "Naive Bayes:\t\t Accuracy: 0.39, Recall: 0.63, F1_score: 0.45, precision: 0.35\n",
      "Logistic Regression:\t\t Accuracy: 0.53, Recall: 0.30, F1_score: 0.34, precision: 0.39\n",
      "75\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "names = [\"Nearest Neighbors\", \"Linear SVM\", \"RBF SVM\", \"Gaussian Process\",\n",
    "         \"Decision Tree\", \"Random Forest\", \"Neural Net\", \"AdaBoost\",\n",
    "         \"Naive Bayes\",\n",
    "         #\"QDA\", \n",
    "         \"Logistic Regression\"\n",
    "        ]\n",
    "\n",
    "classifiers = [\n",
    "    KNeighborsClassifier(3),\n",
    "    SVC(kernel=\"linear\", C=0.025),\n",
    "    SVC(gamma=2, C=1),\n",
    "    GaussianProcessClassifier(1.0 * RBF(1.0)),\n",
    "    DecisionTreeClassifier(max_depth=5),\n",
    "    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),\n",
    "    MLPClassifier(alpha=1, max_iter=1000),\n",
    "    AdaBoostClassifier(),\n",
    "    GaussianNB(),\n",
    "    #QuadraticDiscriminantAnalysis(),\n",
    "    LogisticRegression()\n",
    "]\n",
    "\n",
    "f1_scores = []\n",
    "print(\"size of test df: %d\" % len(test_df))\n",
    "\n",
    "for name,clf in zip(names,classifiers):\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_test = []\n",
    "    y_pred = []\n",
    "    for i,d in test_df.iterrows():\n",
    "        tree = get_grammar_tree(d['USER STORIES'].strip())\n",
    "        pronouns = [p.strip() for p in d['PRONOUNS'] ]\n",
    "        pronoun_positions = get_pronoun_position(tree, pronouns)\n",
    "        nouns = [i for i,(t,p) in enumerate(tree) if p.startswith('NN')]\n",
    "        X = []\n",
    "        y = []\n",
    "        \n",
    "        for p in pronoun_positions:\n",
    "            if d['AMBIGUITY'] == 'no':\n",
    "                X.append(get_feature_vector(tree, p, nouns, pronoun_positions))\n",
    "                y.append(0)\n",
    "            else: \n",
    "                X.append(get_feature_vector(tree, p, nouns, pronoun_positions))\n",
    "                y.append(1)\n",
    "        y_pred_temp = clf.predict(X)\n",
    "        \n",
    "        if 1 in y_pred_temp:\n",
    "            y_pred.append(1)\n",
    "        else:\n",
    "            y_pred.append(0)\n",
    "            \n",
    "        if 1 in y:\n",
    "            y_test.append(1)\n",
    "        else:\n",
    "            y_test.append(0)\n",
    "        \n",
    "        \n",
    "    print(\"%s:\\t\\t Accuracy: %.2f, Recall: %.2f, F1_score: %.2f, precision: %.2f\" % (name, accuracy_score(y_test, y_pred),recall_score(y_test, y_pred),f1_score(y_test, y_pred), precision_score(y_test,y_pred)))\n",
    "    f1_scores.append(f1_score(y_test, y_pred))\n",
    "print(len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "86d4f954",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1,\n",
       "       1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1,\n",
       "       0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,\n",
       "       1, 1, 0, 1, 1, 0, 0, 1, 0])"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.asarray(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "933ad6bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As a Consumer, I want to know that the data I am downloading is good and can be relied on, so that that I don't have to check it myself or run into annoying bugs later on.\n",
      "[]\n",
      "['a Consumer', 'the data', 'bugs']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /Users/emreeren/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('As', 'IN'),\n",
       " ('a Consumer', 'NNP'),\n",
       " (',', ','),\n",
       " ('I', 'PRP'),\n",
       " ('want', 'VBP'),\n",
       " ('to', 'TO'),\n",
       " ('know', 'VB'),\n",
       " ('that', 'IN'),\n",
       " ('the data', 'NN'),\n",
       " ('I', 'PRP'),\n",
       " ('am', 'VBP'),\n",
       " ('downloading', 'VBG'),\n",
       " ('is', 'VBZ'),\n",
       " ('good', 'JJ'),\n",
       " ('and', 'CC'),\n",
       " ('can', 'MD'),\n",
       " ('be', 'VB'),\n",
       " ('relied', 'VBN'),\n",
       " ('on', 'IN'),\n",
       " (',', ','),\n",
       " ('so', 'IN'),\n",
       " ('that', 'IN'),\n",
       " ('that', 'DT'),\n",
       " ('I', 'PRP'),\n",
       " ('do', 'VBP'),\n",
       " (\"n't\", 'RB'),\n",
       " ('have', 'VB'),\n",
       " ('to', 'TO'),\n",
       " ('check', 'VB'),\n",
       " ('it', 'PRP'),\n",
       " ('myself', 'PRP'),\n",
       " ('or', 'CC'),\n",
       " ('run', 'VB'),\n",
       " ('into', 'IN'),\n",
       " ('annoying', 'VBG'),\n",
       " ('bugs', 'NNS'),\n",
       " ('later', 'RB'),\n",
       " ('on', 'IN'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "import nltk\n",
    "nltk.download('brown')\n",
    "text = df.loc[3]['USER STORIES']\n",
    "text= \"As a Platform administrator, I want to be able to translate the data types hierarchies of the Viewer while in embed mode, So that my users can understand the interface in their native language.\"\n",
    "text = \"As a Developer, I want a jQuery plugin for Core Data Packages, so that I can use it to apply to form control that uses a core dataset for autocompletion.\"\n",
    "text = \"As a site visitor, I want to see a list of all upcoming Other Courses and can page through them if necessary, so that I can choose the best course for me.\"\n",
    "text = \"As a trainer, I want to update one of my existing courses or events, so that it reflects accurate information.\"\n",
    "text = \"As a Staff member, I want to create each condo unit, so that I can associate it to the child parcel and address after the condominiums are recorded in the system.\"\n",
    "text = \"As a moderator, I want to edit an item in the list of items to be estimated, so that I can make it better reflect the team's understanding of the item.\"\n",
    "text = \"As a Consumer, I want to know that the data I am downloading is good and can be relied on, so that that I don't have to check it myself or run into annoying bugs later on.\"\n",
    "blob = TextBlob(text)\n",
    "\n",
    "print(text)\n",
    "print(blob.noun_phrases)\n",
    "\n",
    "nouns = [t for i,(t,p) in enumerate(get_grammar_tree(text)) if p.startswith('NN')]\n",
    "print(nouns)\n",
    "get_grammar_tree(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "floppy-container",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       I want the publish button in FABS to deactiva...\n",
       "1       I want the DUNS validations to accept records...\n",
       "2       I want the header information box to show upd...\n",
       "3       I want to successfully Conduct a Plan Review ...\n",
       "4       I want to Review Plans, so that I can review ...\n",
       "                             ...                        \n",
       "244     I want to know what the intellectual value of...\n",
       "245     I want to have a mechanism to obtain a listin...\n",
       "246     I want to have files adequately described, so...\n",
       "247     I want to assess the probability/weight of a ...\n",
       "248     I want to recommend different projects to vol...\n",
       "Name: USER STORIES, Length: 248, dtype: object"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['USER STORIES']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "falling-savage",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"a\".isupper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "brave-technician",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"the\" in \"adatheasd\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "preliminary-deviation",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
